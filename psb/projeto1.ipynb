{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carregamento e preparação de *datasets*\n",
    "\n",
    "O carregamento e preparação de *datasets* é um ótimo exercício para tomarmos conhecimento das ferramentas a serem utilizadas para o processamento de sinais em `python`, seja sinais biológicos quanto de outra natureza, como um som, corrente elétrica, etc.\n",
    "\n",
    "Nesta `notebook` será apresentado o carregamento de um *dataset* público do *website* `UCI - Machine Learning Repository`. O *dataset* a ser utilizado é o `EEG Database Data Set` (https://archive.ics.uci.edu/ml/datasets/EEG+Database).\n",
    "\n",
    "\n",
    "## Descrição do *dataset*:\n",
    "\n",
    "A intenção deste *dataset* é examinar por meio de algoritmos de inteligência computacional a pré-disposição genética que um paciente possui ao alcoolismo.\n",
    "\n",
    "Os principais dados analizados são do tipo *time-series*, em outras palavras, conjuntos de dados que representam um sinal mensurado no domínio do tempo. Os dados são completados com outros atributos como o nome do eletrodo, o número da amostra, etc. Outras informações relevantes do *dataset*:\n",
    "\n",
    "- Quantidade de atributos: 4\n",
    "- Número de instancias: 122\n",
    "- Existem dados faltantes? Sim\n",
    "- Tipos de dados encontrados: categórico, inteiro e real\n",
    "\n",
    "Cada sessão (*trial*) é armazenada da seguinte forma:\n",
    "\n",
    "```\n",
    "# co2a0000364.rd \n",
    "# 120 trials, 64 chans, 416 samples 368 post_stim samples \n",
    "# 3.906000 msecs uV \n",
    "# S1 obj , trial 0 \n",
    "# FP1 chan 0 \n",
    "0 FP1 0 -8.921 \n",
    "0 FP1 1 -8.433 \n",
    "0 FP1 2 -2.574 \n",
    "0 FP1 3 5.239 \n",
    "0 FP1 4 11.587 \n",
    "0 FP1 5 14.028\n",
    "...\n",
    "```\n",
    "\n",
    "As primeiras 4 linhas são de cabeçalho:\n",
    "\n",
    "**linha 1**: identificação do paciente e se ele indica ser um alcoólatra (a) ou controle (c) pela quarta letra (co2**a**0000364);\n",
    "\n",
    "**linha 4**: determina se o paciente foi exposto a um único estímulo (`S1 obj`), a dois estímulos iguais (`S2 match`) ou a dois estímulos diferentes (`S2 no match`);\n",
    "\n",
    "**linha 5**: identifica o início da coleta dos dados pelo eletrodo FP1. As 4 colunas são:\n",
    "\n",
    "```\n",
    "número_da_sessão identificação_do_eletrodo número_da_amostra valor_em_micro_volts\n",
    "```\n",
    "\n",
    "\n",
    "### Realizando o download \n",
    "\n",
    "Primeiro faremos um código para verificar se o *dataset* já foi baixado, caso contrário, executar o código de download:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset já baixado!\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen, urlretrieve\n",
    "import os\n",
    "\n",
    "\n",
    "urls = {\n",
    "    'small': 'https://archive.ics.uci.edu/ml/machine-learning-databases/eeg-mld/smni_eeg_data.tar.gz',\n",
    "    'large_train': 'https://archive.ics.uci.edu/ml/machine-learning-databases/eeg-mld/SMNI_CMI_TRAIN.tar.gz',\n",
    "    'large_test': 'https://archive.ics.uci.edu/ml/machine-learning-databases/eeg-mld/SMNI_CMI_TEST.tar.gz',\n",
    "    'full': 'https://archive.ics.uci.edu/ml/machine-learning-databases/eeg-mld/eeg_full.tar'\n",
    "}\n",
    "\n",
    "# verifica se o diretório dos datasets existe\n",
    "if not os.path.exists('dataset/'):\n",
    "    os.mkdir('dataset/')\n",
    "    for k, v in urls.items():\n",
    "        fn = v.split('/')[-1]\n",
    "        print('Baixando:', fn, '...')\n",
    "        urlretrieve(v, './dataset/{}'.format(fn))\n",
    "    print('Downlod dos datasets concluído!')\n",
    "else:\n",
    "    print('Dataset já baixado!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descompactando pastas e subpastas\n",
    "\n",
    "Agora é necessário descompactar (recursivamente) diversas pastas e subpastas em arquivos GZip. Algumas pastas estão com o arquivo na extensão `.tar`, já outras, `.tar.gz`. Não obstante, algumas subpastas estão compactadas e outras não."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subprocess import getoutput as gop\n",
    "import glob\n",
    "\n",
    "def descompacta():\n",
    "    # único arquivo somente empacotado (tar)\n",
    "    os.mkdir('dataset/eeg_full/')\n",
    "    gop('tar -xvf dataset/eeg_full.tar -C dataset/eeg_full')\n",
    "    os.remove('dataset/eeg_full.tar')\n",
    "\n",
    "    while glob.glob('dataset/**/*.gz', recursive=True):\n",
    "        # quando o arquivo está empacotado (tar) e compactado (gz)\n",
    "        for f in glob.iglob('dataset/**/*.tar.gz', recursive=True):\n",
    "            gop('tar -zxvf {} -C {}'.format(f, f[:f.rindex('/')]))\n",
    "            os.remove(f)\n",
    "        # quando o arquivo está somente compactado (gz)\n",
    "        for f in glob.iglob('dataset/**/*.gz', recursive=True):\n",
    "            gop('gzip -d {}'.format(f))\n",
    "    print('Descompactações finalizadas!')\n",
    "\n",
    "#descompata()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregando parte do dataset\n",
    "\n",
    "Vamos agora carregar o subconjunto \"small\" do *dataset* e fica como <font color='red'>**tarefa de casa**</font> carregar e preparar todos os outros subconjuntos..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def carrega():\n",
    "    # organizando melhor as pastas\n",
    "    os.rename('dataset/smni_eeg_data', 'dataset/small')\n",
    "    os.rename('dataset/eeg_full', 'dataset/full')\n",
    "    os.rename('dataset/SMNI_CMI_TRAIN/', 'dataset/large_train/')\n",
    "    os.rename('dataset/SMNI_CMI_TEST/', 'dataset/large_test/')\n",
    "    print(gop('ls -l dataset/'))\n",
    "\n",
    "#carrega()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import search\n",
    "import numpy as np\n",
    "\n",
    "# identificando pastas\n",
    "folders = {\n",
    "    'small': 'dataset/small',\n",
    "    'large_train': 'dataset/large_train',\n",
    "    'large_test': 'dataset/large_test',\n",
    "    'full': 'dataset/full',\n",
    "}\n",
    "\n",
    "def carrega():\n",
    "    # carregando pasta \"small\"\n",
    "    small_dir = gop('ls {}'.format(folders['small'])).split('\\n')\n",
    "    # 1ª dimensão dos dados contendo os sujeitos. Ex.: C_1, a_m, etc\n",
    "    subjects = list()\n",
    "    for types in small_dir:\n",
    "        files = gop('ls {}/{}'.format(folders['small'], types)).split('\\n')\n",
    "        # 2ª dimensão dos dados contendo as sessões (trials)\n",
    "        trials = list()\n",
    "        for f in files:\n",
    "            arquivo = open('{}/{}/{}'.format(folders['small'], types, f))\n",
    "            text = arquivo.readlines()\n",
    "            # 3ª dimensão dos dados contendo os canais (eletrodos)\n",
    "            chs = list()\n",
    "            # 4ª dimensão dos dados contendo os valores em milivolts\n",
    "            values = list()\n",
    "            for line in text:\n",
    "                # ex: \"# FP1 chan 0\"\n",
    "                t = search('\\w{1,3} chan \\d{1,2}', line)\n",
    "                # ex: \"0 FP1 0 -8.921\"\n",
    "                p = search('^\\d{1,2}\\ \\w{1,3}\\ \\d{1,3}\\ (?P<value>.+$)', line)\n",
    "                if p:\n",
    "                    values.append(float(p.group('value')))\n",
    "                # mudou para outro eletrodo\n",
    "                elif t and values:\n",
    "                    chs.append(values)\n",
    "                    values = list()\n",
    "            chs.append(values)\n",
    "            trials.append(chs)\n",
    "            arquivo.close()\n",
    "        subjects.append(trials)\n",
    "    data = np.array(subjects)\n",
    "    print(data.shape)\n",
    "\n",
    "#carrega()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dados carregados...\n",
    "\n",
    "Os dados \"single\" foram dividos da seguinte forma:\n",
    "```\n",
    "[experimentos, triagens, canais, amostras]\n",
    "```\n",
    "formando um `numpy.array` de quatro dimensões.\n",
    "\n",
    "Em seguida, vamos plotar esses dados para \"tentar\" visualizar algum padrão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "\n",
    "def carrega():\n",
    "    d1 = list()\n",
    "    d2 = list()\n",
    "\n",
    "    for e in range(64):\n",
    "        for i, t in enumerate(np.linspace(0, 1, 256)):\n",
    "            d1.append([e, t, data[0][0][e][i]])\n",
    "            d2.append([e, t, data[1][0][e][i]])\n",
    "    d1 = np.array(d1)\n",
    "    d2 = np.array(d2)\n",
    "    x1, y1, z1 = d1[:,0], d1[:,1], d1[:,2]\n",
    "    x2, y2, z2 = d2[:,0], d2[:,1], d2[:,2]\n",
    "\n",
    "    fig = plt.figure()\n",
    "\n",
    "    ax = fig.add_subplot(1, 2, 1, projection='3d')\n",
    "    surf = ax.plot_trisurf(x1, y1, z1, cmap=cm.inferno, linewidth=1)\n",
    "    ax.set_xlabel('Canais')\n",
    "    ax.set_ylabel('Tempo (seg.)')\n",
    "    ax.set_zlabel('Milivolts')\n",
    "\n",
    "    ax = fig.add_subplot(1, 2, 2, projection='3d')\n",
    "    surf = ax.plot_trisurf(x2, y2, z2, cmap=cm.inferno, linewidth=1)\n",
    "    ax.set_xlabel('Canais')\n",
    "    ax.set_ylabel('Tempo (seg.)')\n",
    "    ax.set_zlabel('Milivolts')\n",
    "\n",
    "    fig.colorbar(surf)\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "#carrega()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparando ambiente\n",
    "\n",
    "Para prosseguir, em cada sub-pasta do `large_test` e `large_train` será gerado três pastas para organizar os arquivos em suas respectivas categorias baseadas em tipo de teste, sendo elas: `s1`, `s2` e `s2no`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def processaArquivo(nomepasta, file):\n",
    "    for i in range(0, 3):\n",
    "        file.readline()\n",
    "\n",
    "    return file.readline().replace('#', '').split(',')[0].strip()\n",
    "\n",
    "def copia(arquivos, nomepasta):\n",
    "    for arquivo in arquivos:\n",
    "        try:\n",
    "            shutil.move(arquivo, nomepasta)\n",
    "        except (FileExistsError, shutil.Error):\n",
    "            return\n",
    "\n",
    "def criaPastas(nomepasta, s1, s2, s2no):\n",
    "    try:\n",
    "        os.makedirs(nomepasta + 's1')\n",
    "        os.makedirs(nomepasta + 's2')\n",
    "        os.makedirs(nomepasta + 's2no')\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "\n",
    "    copia(s1, nomepasta + 's1/')\n",
    "    copia(s2, nomepasta + 's2/')\n",
    "    copia(s2no, nomepasta + 's2no/')\n",
    "\n",
    "def organiza(nomepasta):\n",
    "    s1 = []\n",
    "    s2 = []\n",
    "    s2no = []\n",
    "    files = os.listdir(nomepasta)\n",
    "\n",
    "    for fileName in files:\n",
    "        if os.path.isdir(nomepasta + fileName):\n",
    "            continue\n",
    "        else:\n",
    "            file = open(nomepasta + fileName)\n",
    "\n",
    "        resp = processaArquivo(nomepasta, file)\n",
    "        if 'S1' in resp:\n",
    "            s1.append(nomepasta + fileName)\n",
    "        else:\n",
    "            if 'no' in resp:\n",
    "                s2no.append(nomepasta + fileName)\n",
    "            else:\n",
    "                s2.append(nomepasta + fileName)\n",
    "\n",
    "    return s1, s2, s2no\n",
    "\n",
    "def listSubPaths(path):\n",
    "    subPaths = []\n",
    "    for subPath in os.listdir(path):\n",
    "        if os.path.isdir(path + subPath):\n",
    "            subPaths.append(path + subPath + '/')\n",
    "\n",
    "    return subPaths\n",
    "\n",
    "for path1 in ['large_test/', 'large_train/']:\n",
    "    for path2 in listSubPaths('dataset/' + path1):\n",
    "        s1, s2, s2no = organiza(path2)\n",
    "\n",
    "        if s1 and s2 and s2no:\n",
    "            criaPastas(path2, s1, s2, s2no)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As funções a seguir se referem as manipulações dos dados no domínio da frequência."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mne\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mne import set_eeg_reference as car\n",
    "\n",
    "def toCSV(psds, filename, label, writeMode):\n",
    "    file = open(filename, writeMode)\n",
    "    for line in psds:\n",
    "        line = list(line)\n",
    "        for value in line:\n",
    "            file.write(str(value) + ';')\n",
    "\n",
    "    file.write('{};\\n'.format(label))\n",
    "    file.close()\n",
    "\n",
    "def semMedia(pathName):\n",
    "    files = os.listdir(pathName)\n",
    "    resp =[]\n",
    "    for i,file in enumerate(files):\n",
    "        dic = {}\n",
    "        processFile2(open(pathName + file), dic)\n",
    "        resp.append(dic)\n",
    "\n",
    "    return resp\n",
    "\n",
    "def getSumElectodes(pathName):\n",
    "    files = os.listdir(pathName)\n",
    "    qtdElectrodes = len(files)\n",
    "    resp = {}\n",
    "    for file in files:\n",
    "        processFile(open(pathName + file), resp, qtdElectrodes)\n",
    "\n",
    "    return resp\n",
    "\n",
    "def processFile2(file, resp):\n",
    "    for line in file.readlines():\n",
    "        if line.startswith('#'):\n",
    "            continue\n",
    "        # 41 FP2 168 -4.720\n",
    "        line = line.split()\n",
    "        # recupera os valores da linha\n",
    "        electrode, pos, value = line[1:4]\n",
    "        insert2(resp, electrode, int(pos), float(value))\n",
    "\n",
    "def processFile(file, resp, qtdElectrodes):\n",
    "    # para cada linha do arquivo\n",
    "    for line in file.readlines():\n",
    "        if line.startswith('#'):\n",
    "            continue\n",
    "        # 41 FP2 168 -4.720\n",
    "        line = line.split()\n",
    "        # recupera os valores da linha\n",
    "        electrode, pos, value = line[1:4]\n",
    "        insert(resp, electrode, int(pos), float(value), qtdElectrodes)\n",
    "\n",
    "def insert2(resp, electrode, pos, value):\n",
    "    try:\n",
    "        resp[electrode][pos] = value\n",
    "    except KeyError:\n",
    "        resp[electrode] = [0]*256\n",
    "        insert2(resp, electrode, pos, value)\n",
    "\n",
    "def insert(resp, electrode, pos, value, qtdElectrodes):\n",
    "    try:\n",
    "        resp[electrode][pos] += (value*(1/qtdElectrodes))\n",
    "    except KeyError:\n",
    "        resp[electrode] = [0]*256\n",
    "\n",
    "        insert(resp, electrode, pos, value, qtdElectrodes)\n",
    "\n",
    "def viewTemporal(raw):\n",
    "    # Neste primeiro gráfico mostramos o sinal de um eletrodo no domínio do tempo\n",
    "    plt.plot(np.linspace(0, 1, 256), raw.get_data()[0])\n",
    "    plt.xlabel('tempo (s)')\n",
    "    plt.ylabel('Dados EEG (mV/cm²)')\n",
    "\n",
    "def viewFrequencia(raw):\n",
    "    raw.plot_psd()\n",
    "\n",
    "def applyCar(raw):\n",
    "    chs_P = ['CP5', 'C3', 'CP6', 'C4']\n",
    "    inst, data = car(raw, ref_channels=chs_P)\n",
    "    return inst\n",
    "\n",
    "def plotaGrafico(electrodes):\n",
    "    dataInput = []\n",
    "    ch_types = ['eeg'] * 64\n",
    "    ch_names = list(electrodes.keys())\n",
    "\n",
    "    for electrode in ch_names:\n",
    "        dataInput.append(electrodes[electrode])\n",
    "\n",
    "    info = mne.create_info(ch_names=ch_names, sfreq=256, ch_types=ch_types)\n",
    "    raw = mne.io.RawArray(dataInput, info)\n",
    "    # São removidos aqui alguns canais que não parecem ser informações de eletrodos EEG\n",
    "    raw.drop_channels(['X', 'nd', 'Y'])\n",
    "    # Aplicamos a montagem do padrão 10-20 para todos os eletrodos\n",
    "    montage = mne.channels.read_montage('standard_1020')\n",
    "    raw.set_montage(montage)\n",
    "    # Aqui mostramos todos os 61 eletrodos que representam dados EEG\n",
    "    raw2 = applyCar(raw)\n",
    "    return raw2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os códigos a seguir realizam o pré-processamento dos dados, sendo este dividido em duas etapas:\n",
    "    - utiliando a média de cada teste por pessoa (20 entradas); e\n",
    "    - utilizando sem a média (200 entradas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".........................................."
     ]
    }
   ],
   "source": [
    "from mne.time_frequency import psd_welch as pw\n",
    "import numpy\n",
    "\n",
    "def person(pathName):\n",
    "    if 'a' in pathName:\n",
    "        return 'alcohol'\n",
    "    else:\n",
    "        return 'control'\n",
    "\n",
    "categorys = ['train', 'test']\n",
    "mainPath = 'dataset/'\n",
    "\n",
    "pathsS = ['s1', 's2', 's2no']\n",
    "\n",
    "for category in categorys:\n",
    "    secondPath = 'large_{}/'.format(category)\n",
    "    for s in pathsS:\n",
    "        open(mainPath + category + '_' + s.replace('/', '') + '.csv', 'w').close()\n",
    "\n",
    "    for path in os.listdir(mainPath + secondPath):\n",
    "        print('.', end='', flush=True)\n",
    "        if os.path.isfile(mainPath + secondPath + path):\n",
    "            continue\n",
    "\n",
    "        path += '/'\n",
    "\n",
    "        for s in pathsS:\n",
    "            s += '/'\n",
    "            sMedia = semMedia(mainPath + secondPath + path + s)\n",
    "\n",
    "            for sM in sMedia:\n",
    "                psds, freqs = pw(plotaGrafico(sM))\n",
    "\n",
    "                #toCSV(psds[int(len(psds)/2):], mainPath + category + '_' + s.replace('/', '') + '.csv', person(path), 'a')            \n",
    "                toCSV(psds, mainPath + category + '_' + s.replace('/', '') + '.csv', person(path), 'a')            \n",
    "\n",
    "            # media = getSumElectodes(mainPath + secondPath + path + s)\n",
    "            # psds, freqs = pw(plotaGrafico(media))\n",
    "            # toCSV(psds[int(len(psds)/2):], mainPath + category + '_' + s.replace('/', '') + '.csv', person(path), 'a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificador\n",
    "\n",
    "Para cada arquivo categoria de teste aplicada no usuário, é realizado a classificação por dois algoritmos supervisionado, sendo eles o `KNN` e o `SVM`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category: S1\n",
      "KNN: 9\n",
      "Total de treinamento: 200\n",
      "Total de testes: 200\n",
      "Total de acertos: 196\n",
      "Porcentagem de acertos: 98.00%\n",
      "\n",
      "KNN: 7\n",
      "Total de treinamento: 200\n",
      "Total de testes: 200\n",
      "Total de acertos: 195\n",
      "Porcentagem de acertos: 97.50%\n",
      "\n",
      "KNN: 5\n",
      "Total de treinamento: 200\n",
      "Total de testes: 200\n",
      "Total de acertos: 194\n",
      "Porcentagem de acertos: 97.00%\n",
      "\n",
      "SVM:\n",
      "Total de treinamento: 200\n",
      "Total de testes: 200\n",
      "Total de acertos: 193\n",
      "Porcentagem de acertos: 96.50%\n",
      "\n",
      "Category: S2\n",
      "KNN: 9\n",
      "Total de treinamento: 200\n",
      "Total de testes: 200\n",
      "Total de acertos: 194\n",
      "Porcentagem de acertos: 97.00%\n",
      "\n",
      "KNN: 7\n",
      "Total de treinamento: 200\n",
      "Total de testes: 200\n",
      "Total de acertos: 196\n",
      "Porcentagem de acertos: 98.00%\n",
      "\n",
      "KNN: 5\n",
      "Total de treinamento: 200\n",
      "Total de testes: 200\n",
      "Total de acertos: 196\n",
      "Porcentagem de acertos: 98.00%\n",
      "\n",
      "SVM:\n",
      "Total de treinamento: 200\n",
      "Total de testes: 200\n",
      "Total de acertos: 193\n",
      "Porcentagem de acertos: 96.50%\n",
      "\n",
      "Category: S2NO\n",
      "KNN: 9\n",
      "Total de treinamento: 200\n",
      "Total de testes: 200\n",
      "Total de acertos: 193\n",
      "Porcentagem de acertos: 96.50%\n",
      "\n",
      "KNN: 7\n",
      "Total de treinamento: 200\n",
      "Total de testes: 200\n",
      "Total de acertos: 199\n",
      "Porcentagem de acertos: 99.50%\n",
      "\n",
      "KNN: 5\n",
      "Total de treinamento: 200\n",
      "Total de testes: 200\n",
      "Total de acertos: 194\n",
      "Porcentagem de acertos: 97.00%\n",
      "\n",
      "SVM:\n",
      "Total de treinamento: 200\n",
      "Total de testes: 200\n",
      "Total de acertos: 189\n",
      "Porcentagem de acertos: 94.50%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "\n",
    "def extrai(fileName):\n",
    "    x, y = [], []\n",
    "    file = open(fileName)\n",
    "    # pre processing POWER\n",
    "    power = PowerTransformer()\n",
    "    for line in file.readlines():\n",
    "        line = line.replace(';\\n', '').split(';')\n",
    "        y.append(line.pop())\n",
    "        x.append(line)\n",
    "\n",
    "    return power.fit_transform(np.array(x)), np.array(y)\n",
    "\n",
    "def printResult(train_values, test_values, acertos, k=False):\n",
    "    if k:\n",
    "        print('KNN:', k)\n",
    "    else:\n",
    "        print('SVM:')\n",
    "\n",
    "    print('Total de treinamento: %d' % len(train_values))\n",
    "    print('Total de testes: %d' % (len(test_values)))\n",
    "    print('Total de acertos: %d' % acertos)\n",
    "    print('Porcentagem de acertos: %.2f%%' % (100 * acertos / (len(test_values))))\n",
    "    print()\n",
    "\n",
    "def knn(k):\n",
    "    global train_values\n",
    "    global test_values\n",
    "\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(train_values, train_label)\n",
    "    rotulos_previstos = knn.predict(test_values)\n",
    "    acertos, indice_rotulo = 0, 0\n",
    "    for i in range(0, len(test_values)):\n",
    "        if rotulos_previstos[indice_rotulo] == test_label[i]:\n",
    "            acertos += 1\n",
    "        indice_rotulo += 1\n",
    "\n",
    "    printResult(train_values, test_values, acertos, k)\n",
    "\n",
    "def svmPSB():\n",
    "    global train_values\n",
    "    global test_values\n",
    "\n",
    "    xTrain = train_values\n",
    "    xTest = test_values\n",
    "\n",
    "    clf = svm.SVC(kernel='linear')\n",
    "    clf.fit(xTrain, train_label)\n",
    "    rotulos_previstos = clf.predict(test_values)\n",
    "    # Contabilizar acertos para os dados de teste\n",
    "    acertos, indice_rotulo = 0, 0\n",
    "    for i in range(0, len(test_values)):\n",
    "        if rotulos_previstos[indice_rotulo] == test_label[i]:\n",
    "            acertos += 1\n",
    "        indice_rotulo += 1\n",
    "\n",
    "    printResult(train_values, test_values, acertos)\n",
    "\n",
    "for s in ['s1', 's2', 's2no']:\n",
    "    print(\"Category: {}\".format(s.upper()))\n",
    "\n",
    "    fileNameTest = './dataset/test_{}.csv'.format(s)\n",
    "    fileNameTrain = './dataset/train_{}.csv'.format(s)\n",
    "\n",
    "    test_values, test_label = extrai(fileNameTest)\n",
    "    train_values, train_label = extrai(fileNameTrain)\n",
    "\n",
    "    knn(9)\n",
    "    knn(7)\n",
    "    knn(5)\n",
    "\n",
    "    svmPSB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resultados\n",
    "\n",
    "A seguir, está a tabela com a taxa de acerto dos classificadores `KNN` e `SMV`:\n",
    "\n",
    "\n",
    "Category/KNN | k=5 | K=7 | K=9 |\n",
    "---- | ---- | ---- | ---- |\n",
    "**S1** | 97% | 97.5% | `98%`\n",
    "**S2** | `98%` | `98%` | 97%\n",
    "**S2NO** | 97% | `99.5%` | 96.5%\n",
    "\n",
    "Category/SVM | LINEAR |\n",
    "---- | ---- |\n",
    "**S1** | 96.5% |\n",
    "**S2** | 96.5% |\n",
    "**S2NO** | 94.5% |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
